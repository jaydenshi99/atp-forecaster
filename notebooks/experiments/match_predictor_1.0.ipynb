{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "390c57d2-2c33-4f84-ba3e-482f87c18330",
   "metadata": {},
   "source": [
    "# 1.0\n",
    "This model will use a simple XGBoost on the constructed features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b190f2de-d16e-4e28-a3c3-87a50b1dae00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['draw_size', 'best_of', 'total_matches_a', 'total_matches_b',\n",
       "       'total_surface_matches_a', 'total_surface_matches_b',\n",
       "       'recent_matches_a', 'recent_matches_b', 'recent_minutes_a',\n",
       "       'recent_minutes_b', 'hth_win_p_a', 'hth_matches', 'form_delta_a',\n",
       "       'form_delta_b', 'elo_momentum_a', 'elo_momentum_b', 'surface_Carpet',\n",
       "       'surface_Clay', 'surface_Grass', 'surface_Hard', 'tourney_level_A',\n",
       "       'tourney_level_F', 'tourney_level_G', 'tourney_level_M', 'hand_a_A',\n",
       "       'hand_a_L', 'hand_a_R', 'hand_a_U', 'hand_b_A', 'hand_b_L', 'hand_b_R',\n",
       "       'hand_b_U', 'round_BR', 'round_ER', 'round_F', 'round_QF', 'round_R128',\n",
       "       'round_R16', 'round_R32', 'round_R64', 'round_RR', 'round_SF',\n",
       "       'height_diff', 'age_diff', 'elo_diff', 'elo_surface_diff', 'p_ace_diff',\n",
       "       'p_df_diff', 'p_1stIn_diff', 'p_1stWon_diff', 'p_2ndWon_diff',\n",
       "       'p_2ndWon_inPlay_diff', 'p_bpSaved_diff', 'p_rpw_diff',\n",
       "       'p_retAceAgainst_diff', 'p_ret1stWon_diff', 'p_ret2ndWon_diff',\n",
       "       'p_ret2ndWon_inPlay_diff', 'p_bpConv_diff', 'p_totalPtsWon_diff',\n",
       "       'dominance_ratio_diff', 'inv_rank_diff', 'log_rank_points_diff',\n",
       "       'result'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet('../../data/training_data/dataset_v1.parquet')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ab93162-3e24-4661-a5aa-038dd604b83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, log_loss\n",
    "import numpy as np\n",
    "\n",
    "def time_series_cv(df, model, n_splits=100, debug=True):\n",
    "    X = df.drop(columns=[\"result\"])\n",
    "    y = df[\"result\"].values\n",
    "\n",
    "    fold_size = len(df) // (n_splits + 1)\n",
    "    aucs, accs, losses = [], [], []\n",
    "\n",
    "    for i in range(1, n_splits + 1):\n",
    "        train_end = fold_size * i\n",
    "        X_train, y_train = X.iloc[:train_end], y[:train_end]\n",
    "        X_test,  y_test  = X.iloc[train_end:train_end + fold_size], y[train_end:train_end + fold_size]\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict_proba(X_test)[:,1]\n",
    "        \n",
    "        auc = roc_auc_score(y_test, preds)\n",
    "        pred_class = (preds >= 0.5).astype(int)\n",
    "        acc = accuracy_score(y_test, pred_class)\n",
    "        loss = log_loss(y_test, preds)\n",
    "\n",
    "        aucs.append(auc)\n",
    "        accs.append(acc)\n",
    "        losses.append(loss)\n",
    "\n",
    "        if debug:\n",
    "            print(f\"{i} AUC={auc:.4f}, Accuracy={acc:.4f}, LogLoss={loss:.4f}\")\n",
    "\n",
    "    if debug:\n",
    "        print(f\"\\nAUC:      mean={np.mean(aucs):.4f}, std={np.std(aucs):.4f}\")\n",
    "        print(f\"Accuracy: mean={np.mean(accs):.4f}, std={np.std(accs):.4f}\")\n",
    "        print(f\"LogLoss:  mean={np.mean(losses):.4f}, std={np.std(losses):.4f}\")\n",
    "\n",
    "    return aucs, accs, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "600411c0-19bd-4e36-ab70-6cf5e8a89d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier(\n",
    "    n_estimators=723,\n",
    "    learning_rate=0.0073542448230937306,\n",
    "    max_depth=5,\n",
    "    subsample=0.7107128168724214,\n",
    "    colsample_bytree=0.5088728633198565,\n",
    "    min_child_weight=8,\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "# aucs, accs, losses = time_series_cv(df, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535693d5-f75e-49a8-9db8-20b5812899b4",
   "metadata": {},
   "source": [
    "With sensible parameters on the XGBoost model, a cross validation of AUC=0.7273 and Accuracy=0.6643 is achieved. The model's performance increased as more data came in as expected. However, as we come across more recent data, the accuracy drops significantly. This drop could be caused by a regime shift in data, resulting from COVID and the retirement of the Big Three.\n",
    "\n",
    "A coarse grid search on the hyperparameters is be performed to find optimal hyperparameters.\n",
    "\n",
    "### Hyperparameter Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1ea3f84-74c2-4ba7-8de8-309f9c1185ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def tune_xgb(df, param_grid, metric=\"auc\"):\n",
    "\n",
    "    keys = list(param_grid.keys())\n",
    "    best_score = -np.inf\n",
    "    best_params = None\n",
    "\n",
    "    for values in product(*param_grid.values()):\n",
    "        params = dict(zip(keys, values))\n",
    "\n",
    "        model = XGBClassifier(\n",
    "            **params,\n",
    "            objective=\"binary:logistic\",\n",
    "            eval_metric=\"logloss\"\n",
    "        )\n",
    "\n",
    "        aucs, _, losses = time_series_cv(df, model, debug=False)\n",
    "\n",
    "        mean_auc = np.mean(aucs)\n",
    "        mean_loss = np.mean(losses)\n",
    "\n",
    "        if metric == \"auc\":\n",
    "            score = mean_auc\n",
    "        else:\n",
    "            score = -mean_loss   # minimise logloss\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_params = params\n",
    "            print(\"\\nBest:\", best_params, \"\\nScore:\", best_score)\n",
    "        else:\n",
    "            print(\"\\nNo Change\")\n",
    "    \n",
    "    return best_params\n",
    "    \n",
    "param_grid = {\n",
    "    \"max_depth\": [4, 5],\n",
    "    \"learning_rate\": [0.02],\n",
    "    \"subsample\": [0.7, 0.9],\n",
    "    \"colsample_bytree\": [0.7, 0.9],\n",
    "    \"min_child_weight\": [1, 5, 10],\n",
    "    \"n_estimators\": [300, 600],\n",
    "}\n",
    "\n",
    "# best_params = tune_xgb(df, param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff83cfc-5722-4d0e-81b8-648f97c3a96c",
   "metadata": {},
   "source": [
    "Instead of a coarse grid search that takes an exponentially long time to run as features increase, we can use bayesian optimisation. Instead of picking them randomly, the sampler uses past trial results to prefer values that previously looked good and avoid values that looked bad, making the search smarter over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f513c31e-987b-427f-9757-88625faa914a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def tune_xgb_optuna(df, n_trials=100, n_splits=20):\n",
    "    \"\"\"\n",
    "    Run Bayesian hyperparameter optimisation for XGBoost\n",
    "    using time-series cross-validation on `df`.\n",
    "\n",
    "    Assumes `time_series_cv(df, model, n_splits)` returns\n",
    "    (aucs, accs, losses) for each fold.\n",
    "    \"\"\"\n",
    "\n",
    "    def objective(trial):\n",
    "        # sample hyperparameters\n",
    "        max_depth = trial.suggest_int(\"max_depth\", 3, 8)\n",
    "        learning_rate = trial.suggest_float(\"learning_rate\", 0.001, 0.1, log=True)\n",
    "        subsample = trial.suggest_float(\"subsample\", 0.4, 1.0)\n",
    "        colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.4, 1.0)\n",
    "        min_child_weight = trial.suggest_int(\"min_child_weight\", 1, 20)\n",
    "        n_estimators = trial.suggest_int(\"n_estimators\", 300, 1000)\n",
    "\n",
    "        model = XGBClassifier(\n",
    "            max_depth=max_depth,\n",
    "            learning_rate=learning_rate,\n",
    "            subsample=subsample,\n",
    "            colsample_bytree=colsample_bytree,\n",
    "            min_child_weight=min_child_weight,\n",
    "            n_estimators=n_estimators,\n",
    "            objective=\"binary:logistic\",\n",
    "            eval_metric=\"logloss\",\n",
    "            tree_method=\"hist\",\n",
    "            random_state=42,\n",
    "        )\n",
    "\n",
    "        aucs, accs, losses = time_series_cv(df, model, n_splits=n_splits, debug=True)\n",
    "\n",
    "        mean_auc = np.mean(aucs)\n",
    "\n",
    "        # maximsed value\n",
    "        return mean_auc\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\")  # maximise AUC\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    print(\"\\nBest AUC:\", study.best_value)\n",
    "    print(\"Best params:\")\n",
    "    for k, v in study.best_params.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "\n",
    "    return study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a692eda-1c07-48a0-bb76-833d9d4017eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# study = tune_xgb_optuna(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
