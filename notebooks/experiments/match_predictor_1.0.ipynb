{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "390c57d2-2c33-4f84-ba3e-482f87c18330",
   "metadata": {},
   "source": [
    "# 1.0\n",
    "This model will use a simple XGBoost on the constructed features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b190f2de-d16e-4e28-a3c3-87a50b1dae00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['draw_size', 'best_of', 'hth_win_p_a', 'surface_Carpet', 'surface_Clay',\n",
       "       'surface_Grass', 'surface_Hard', 'tourney_level_A', 'tourney_level_F',\n",
       "       'tourney_level_G', 'tourney_level_M', 'hand_a_A', 'hand_a_L',\n",
       "       'hand_a_R', 'hand_a_U', 'hand_b_A', 'hand_b_L', 'hand_b_R', 'hand_b_U',\n",
       "       'round_BR', 'round_ER', 'round_F', 'round_QF', 'round_R128',\n",
       "       'round_R16', 'round_R32', 'round_R64', 'round_RR', 'round_SF',\n",
       "       'height_diff', 'age_diff', 'elo_diff', 'elo_surface_diff', 'p_ace_diff',\n",
       "       'p_df_diff', 'p_1stIn_diff', 'p_1stWon_diff', 'p_2ndWon_diff',\n",
       "       'p_2ndWon_inPlay_diff', 'p_bpSaved_diff', 'p_rpw_diff',\n",
       "       'p_retAceAgainst_diff', 'p_ret1stWon_diff', 'p_ret2ndWon_diff',\n",
       "       'p_ret2ndWon_inPlay_diff', 'p_bpConv_diff', 'p_totalPtsWon_diff',\n",
       "       'dominance_ratio_diff', 'ht_diff', 'form_delta_diff',\n",
       "       'elo_momentum_diff', 'recent_minutes_diff', 'log_rank_points_diff',\n",
       "       'log_total_matches_diff', 'log_total_surface_matches_diff',\n",
       "       'log_recent_matches_diff', 'inv_rank_diff', 'log_hth_matches',\n",
       "       'result'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet('../../data/training_data/dataset_v1.parquet')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ab93162-3e24-4661-a5aa-038dd604b83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, log_loss\n",
    "import numpy as np\n",
    "\n",
    "def time_series_cv(df, model, n_splits=100, debug=True):\n",
    "    X = df.drop(columns=[\"result\"])\n",
    "    y = df[\"result\"].values\n",
    "\n",
    "    fold_size = len(df) // (n_splits + 1)\n",
    "    aucs, accs, losses = [], [], []\n",
    "\n",
    "    for i in range(1, n_splits + 1):\n",
    "        train_end = fold_size * i\n",
    "        X_train, y_train = X.iloc[:train_end], y[:train_end]\n",
    "        X_test,  y_test  = X.iloc[train_end:train_end + fold_size], y[train_end:train_end + fold_size]\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict_proba(X_test)[:,1]\n",
    "        \n",
    "        auc = roc_auc_score(y_test, preds)\n",
    "        pred_class = (preds >= 0.5).astype(int)\n",
    "        acc = accuracy_score(y_test, pred_class)\n",
    "        loss = log_loss(y_test, preds)\n",
    "\n",
    "        aucs.append(auc)\n",
    "        accs.append(acc)\n",
    "        losses.append(loss)\n",
    "\n",
    "        if debug:\n",
    "            print(f\"{i} AUC={auc:.4f}, Accuracy={acc:.4f}, LogLoss={loss:.4f}\")\n",
    "\n",
    "    if debug:\n",
    "        print(f\"\\nAUC:      mean={np.mean(aucs):.4f}, std={np.std(aucs):.4f}\")\n",
    "        print(f\"Accuracy: mean={np.mean(accs):.4f}, std={np.std(accs):.4f}\")\n",
    "        print(f\"LogLoss:  mean={np.mean(losses):.4f}, std={np.std(losses):.4f}\")\n",
    "\n",
    "    return aucs, accs, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "600411c0-19bd-4e36-ab70-6cf5e8a89d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 AUC=0.7649, Accuracy=0.6869, LogLoss=0.5800\n",
      "2 AUC=0.8255, Accuracy=0.7445, LogLoss=0.5179\n",
      "3 AUC=0.7687, Accuracy=0.6980, LogLoss=0.5750\n",
      "4 AUC=0.8133, Accuracy=0.7345, LogLoss=0.5290\n",
      "5 AUC=0.8185, Accuracy=0.7445, LogLoss=0.5229\n",
      "6 AUC=0.8388, Accuracy=0.7533, LogLoss=0.5048\n",
      "7 AUC=0.7974, Accuracy=0.7146, LogLoss=0.5448\n",
      "8 AUC=0.8304, Accuracy=0.7600, LogLoss=0.5086\n",
      "9 AUC=0.8291, Accuracy=0.7522, LogLoss=0.5108\n",
      "10 AUC=0.8254, Accuracy=0.7511, LogLoss=0.5190\n",
      "11 AUC=0.8464, Accuracy=0.7688, LogLoss=0.4918\n",
      "12 AUC=0.8054, Accuracy=0.7345, LogLoss=0.5366\n",
      "13 AUC=0.8301, Accuracy=0.7600, LogLoss=0.5099\n",
      "14 AUC=0.8208, Accuracy=0.7600, LogLoss=0.5209\n",
      "15 AUC=0.8304, Accuracy=0.7611, LogLoss=0.5111\n",
      "16 AUC=0.7891, Accuracy=0.7223, LogLoss=0.5554\n",
      "17 AUC=0.8337, Accuracy=0.7423, LogLoss=0.5039\n",
      "18 AUC=0.8099, Accuracy=0.7312, LogLoss=0.5321\n",
      "19 AUC=0.8119, Accuracy=0.7467, LogLoss=0.5296\n",
      "20 AUC=0.7932, Accuracy=0.7268, LogLoss=0.5514\n",
      "21 AUC=0.8126, Accuracy=0.7312, LogLoss=0.5293\n",
      "22 AUC=0.7959, Accuracy=0.7212, LogLoss=0.5471\n",
      "23 AUC=0.7936, Accuracy=0.7190, LogLoss=0.5498\n",
      "24 AUC=0.7956, Accuracy=0.7190, LogLoss=0.5458\n",
      "25 AUC=0.7847, Accuracy=0.7279, LogLoss=0.5628\n",
      "26 AUC=0.7873, Accuracy=0.6991, LogLoss=0.5559\n",
      "27 AUC=0.8179, Accuracy=0.7412, LogLoss=0.5241\n",
      "28 AUC=0.8153, Accuracy=0.7301, LogLoss=0.5246\n",
      "29 AUC=0.7714, Accuracy=0.7091, LogLoss=0.5714\n",
      "30 AUC=0.7776, Accuracy=0.7058, LogLoss=0.5661\n",
      "31 AUC=0.7830, Accuracy=0.7146, LogLoss=0.5621\n",
      "32 AUC=0.7949, Accuracy=0.7201, LogLoss=0.5472\n",
      "33 AUC=0.7960, Accuracy=0.7146, LogLoss=0.5472\n",
      "34 AUC=0.8142, Accuracy=0.7356, LogLoss=0.5287\n",
      "35 AUC=0.8117, Accuracy=0.7334, LogLoss=0.5300\n",
      "36 AUC=0.7915, Accuracy=0.7312, LogLoss=0.5532\n",
      "37 AUC=0.8073, Accuracy=0.7279, LogLoss=0.5332\n",
      "38 AUC=0.7605, Accuracy=0.6947, LogLoss=0.5851\n",
      "39 AUC=0.8238, Accuracy=0.7356, LogLoss=0.5233\n",
      "40 AUC=0.8172, Accuracy=0.7323, LogLoss=0.5223\n",
      "41 AUC=0.8043, Accuracy=0.7489, LogLoss=0.5389\n",
      "42 AUC=0.8092, Accuracy=0.7434, LogLoss=0.5359\n",
      "43 AUC=0.7882, Accuracy=0.7024, LogLoss=0.5552\n",
      "44 AUC=0.7841, Accuracy=0.7235, LogLoss=0.5609\n",
      "45 AUC=0.8108, Accuracy=0.7356, LogLoss=0.5339\n",
      "46 AUC=0.8337, Accuracy=0.7611, LogLoss=0.5094\n",
      "47 AUC=0.8146, Accuracy=0.7389, LogLoss=0.5306\n",
      "48 AUC=0.7999, Accuracy=0.7157, LogLoss=0.5428\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      1\u001b[39m model = XGBClassifier(\n\u001b[32m      2\u001b[39m     n_estimators=\u001b[32m723\u001b[39m,\n\u001b[32m      3\u001b[39m     learning_rate=\u001b[32m0.0073542448230937306\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m      9\u001b[39m     eval_metric=\u001b[33m'\u001b[39m\u001b[33mlogloss\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     10\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m aucs, accs, losses = \u001b[43mtime_series_cv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mtime_series_cv\u001b[39m\u001b[34m(df, model, n_splits, debug)\u001b[39m\n\u001b[32m     14\u001b[39m X_train, y_train = X.iloc[:train_end], y[:train_end]\n\u001b[32m     15\u001b[39m X_test,  y_test  = X.iloc[train_end:train_end + fold_size], y[train_end:train_end + fold_size]\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m preds = model.predict_proba(X_test)[:,\u001b[32m1\u001b[39m]\n\u001b[32m     20\u001b[39m auc = roc_auc_score(y_test, preds)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/xgboost/core.py:774\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    773\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/xgboost/sklearn.py:1803\u001b[39m, in \u001b[36mXGBClassifier.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[39m\n\u001b[32m   1783\u001b[39m evals_result: EvalsLog = {}\n\u001b[32m   1784\u001b[39m train_dmatrix, evals = _wrap_evaluation_matrices(\n\u001b[32m   1785\u001b[39m     missing=\u001b[38;5;28mself\u001b[39m.missing,\n\u001b[32m   1786\u001b[39m     X=X,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1800\u001b[39m     feature_types=feature_types,\n\u001b[32m   1801\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1803\u001b[39m \u001b[38;5;28mself\u001b[39m._Booster = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1804\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1805\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1806\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1807\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1808\u001b[39m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1809\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1810\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1811\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1812\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1813\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1814\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1815\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1817\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m.objective):\n\u001b[32m   1818\u001b[39m     \u001b[38;5;28mself\u001b[39m.objective = params[\u001b[33m\"\u001b[39m\u001b[33mobjective\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/xgboost/core.py:774\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    773\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/xgboost/training.py:199\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(params, dtrain, num_boost_round, evals, obj, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cb_container.before_iteration(bst, i, dtrain, evals):\n\u001b[32m    198\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m \u001b[43mbst\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cb_container.after_iteration(bst, i, dtrain, evals):\n\u001b[32m    201\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/xgboost/core.py:2434\u001b[39m, in \u001b[36mBooster.update\u001b[39m\u001b[34m(self, dtrain, iteration, fobj)\u001b[39m\n\u001b[32m   2430\u001b[39m \u001b[38;5;28mself\u001b[39m._assign_dmatrix_features(dtrain)\n\u001b[32m   2432\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2433\u001b[39m     _check_call(\n\u001b[32m-> \u001b[39m\u001b[32m2434\u001b[39m         \u001b[43m_LIB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2435\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\n\u001b[32m   2436\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2437\u001b[39m     )\n\u001b[32m   2438\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2439\u001b[39m     pred = \u001b[38;5;28mself\u001b[39m.predict(dtrain, output_margin=\u001b[38;5;28;01mTrue\u001b[39;00m, training=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model = XGBClassifier(\n",
    "    n_estimators=723,\n",
    "    learning_rate=0.0073542448230937306,\n",
    "    max_depth=5,\n",
    "    subsample=0.7107128168724214,\n",
    "    colsample_bytree=0.5088728633198565,\n",
    "    min_child_weight=8,\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "aucs, accs, losses = time_series_cv(df, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535693d5-f75e-49a8-9db8-20b5812899b4",
   "metadata": {},
   "source": [
    "With sensible parameters on the XGBoost model, a cross validation of AUC=0.7273 and Accuracy=0.6643 is achieved. The model's performance increased as more data came in as expected. However, as we come across more recent data, the accuracy drops significantly. This drop could be caused by a regime shift in data, resulting from COVID and the retirement of the Big Three.\n",
    "\n",
    "A coarse grid search on the hyperparameters is performed to find optimal hyperparameters.\n",
    "\n",
    "### Hyperparameter Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1ea3f84-74c2-4ba7-8de8-309f9c1185ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def tune_xgb(df, param_grid, metric=\"auc\"):\n",
    "\n",
    "    keys = list(param_grid.keys())\n",
    "    best_score = -np.inf\n",
    "    best_params = None\n",
    "\n",
    "    for values in product(*param_grid.values()):\n",
    "        params = dict(zip(keys, values))\n",
    "\n",
    "        model = XGBClassifier(\n",
    "            **params,\n",
    "            objective=\"binary:logistic\",\n",
    "            eval_metric=\"logloss\"\n",
    "        )\n",
    "\n",
    "        aucs, _, losses = time_series_cv(df, model, debug=False)\n",
    "\n",
    "        mean_auc = np.mean(aucs)\n",
    "        mean_loss = np.mean(losses)\n",
    "\n",
    "        if metric == \"auc\":\n",
    "            score = mean_auc\n",
    "        else:\n",
    "            score = -mean_loss   # minimise logloss\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_params = params\n",
    "            print(\"\\nBest:\", best_params, \"\\nScore:\", best_score)\n",
    "        else:\n",
    "            print(\"\\nNo Change\")\n",
    "    \n",
    "    return best_params\n",
    "    \n",
    "param_grid = {\n",
    "    \"max_depth\": [4, 5],\n",
    "    \"learning_rate\": [0.02],\n",
    "    \"subsample\": [0.7, 0.9],\n",
    "    \"colsample_bytree\": [0.7, 0.9],\n",
    "    \"min_child_weight\": [1, 5, 10],\n",
    "    \"n_estimators\": [300, 600],\n",
    "}\n",
    "\n",
    "# best_params = tune_xgb(df, param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff83cfc-5722-4d0e-81b8-648f97c3a96c",
   "metadata": {},
   "source": [
    "Instead of a coarse grid search that takes an exponentially long time to run as features increase, we can use bayesian optimisation. Instead of picking them randomly, the sampler uses past trial results to prefer values that previously looked good and avoid values that looked bad, making the search smarter over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f513c31e-987b-427f-9757-88625faa914a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def tune_xgb_optuna(df, n_trials=100, n_splits=20):\n",
    "    \"\"\"\n",
    "    Run Bayesian hyperparameter optimisation for XGBoost\n",
    "    using time-series cross-validation on `df`.\n",
    "\n",
    "    Assumes `time_series_cv(df, model, n_splits)` returns\n",
    "    (aucs, accs, losses) for each fold.\n",
    "    \"\"\"\n",
    "\n",
    "    def objective(trial):\n",
    "        # sample hyperparameters\n",
    "        max_depth = trial.suggest_int(\"max_depth\", 3, 8)\n",
    "        learning_rate = trial.suggest_float(\"learning_rate\", 0.001, 0.1, log=True)\n",
    "        subsample = trial.suggest_float(\"subsample\", 0.4, 1.0)\n",
    "        colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.4, 1.0)\n",
    "        min_child_weight = trial.suggest_int(\"min_child_weight\", 1, 20)\n",
    "        n_estimators = trial.suggest_int(\"n_estimators\", 300, 1000)\n",
    "\n",
    "        model = XGBClassifier(\n",
    "            max_depth=max_depth,\n",
    "            learning_rate=learning_rate,\n",
    "            subsample=subsample,\n",
    "            colsample_bytree=colsample_bytree,\n",
    "            min_child_weight=min_child_weight,\n",
    "            n_estimators=n_estimators,\n",
    "            objective=\"binary:logistic\",\n",
    "            eval_metric=\"logloss\",\n",
    "            tree_method=\"hist\",\n",
    "            random_state=42,\n",
    "        )\n",
    "\n",
    "        aucs, accs, losses = time_series_cv(df, model, n_splits=n_splits, debug=True)\n",
    "\n",
    "        mean_auc = np.mean(aucs)\n",
    "\n",
    "        # maximsed value\n",
    "        return mean_auc\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\")  # maximise AUC\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    print(\"\\nBest AUC:\", study.best_value)\n",
    "    print(\"Best params:\")\n",
    "    for k, v in study.best_params.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "\n",
    "    return study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a692eda-1c07-48a0-bb76-833d9d4017eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# study = tune_xgb_optuna(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
